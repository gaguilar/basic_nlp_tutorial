{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Sequence to Sequence Models (COSC 6336)\n",
    "### Solution of HW3\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right; font-size: large;\"><i>authored by Gustavo Aguilar</i></div>\n",
    "<div style=\"text-align: right; font-size: small;\"><i>March 19, 2020</i></div>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Vocab: ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Decoder Vocab: ['<pad>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<start>', '<stop>']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, vocab_list):\n",
    "        self.itos = vocab_list\n",
    "        self.stoi = {d:i for i, d in enumerate(self.itos)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)  \n",
    "    \n",
    "def sorting_letters_dataset(size):\n",
    "    dataset = []\n",
    "    for _ in range(size):\n",
    "        x = []\n",
    "        for _ in range(random.randint(3, 8)):\n",
    "            letter = chr(random.randint(97, 122))\n",
    "            repeat = [letter] * random.randint(1, 3)\n",
    "            x.extend(repeat)\n",
    "        y = sorted(set(x))\n",
    "        dataset.append((x, y))\n",
    "    return zip(*dataset)\n",
    "\n",
    "src_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)])\n",
    "tgt_vocab = Vocab(['<pad>'] + [chr(i+97) for i in range(26)] + ['<start>', '<stop>'] )\n",
    "\n",
    "train_inp, train_out = sorting_letters_dataset(20_000)\n",
    "valid_inp, valid_out = sorting_letters_dataset(5_000)\n",
    "\n",
    "print(\"Encoder Vocab:\", src_vocab.itos)\n",
    "print(\"Decoder Vocab:\", tgt_vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings: use the mapped indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_elems(elems, mapper):\n",
    "    return [mapper[elem] for elem in elems]\n",
    "\n",
    "def map_many_elems(many_elems, mapper):\n",
    "    return [map_elems(elems, mapper) for elems in many_elems]\n",
    "\n",
    "def add_start_stop(start, stop, many_elems):\n",
    "    return [[start] + elems + [stop] for elems in many_elems]\n",
    "\n",
    "train_x = map_many_elems(train_inp, src_vocab.stoi)\n",
    "train_y = map_many_elems(train_out, tgt_vocab.stoi)\n",
    "\n",
    "train_y = add_start_stop(tgt_vocab.stoi['<start>'], tgt_vocab.stoi['<stop>'], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 9, 16, 16, 16, 16, 16] ['s', 'i', 'p', 'p', 'p', 'p', 'p']\n",
      "[27, 9, 16, 19, 28] ['i', 'p', 's']\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0], train_inp[0])\n",
    "print(train_y[0], train_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaguil20/.conda/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (emb): Embedding(27, 64)\n",
       "    (lstm): LSTM(64, 128, dropout=0.5)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): Embedding(29, 64)\n",
       "    (lstm): LSTM(64, 128, dropout=0.5)\n",
       "    (clf): Linear(in_features=128, out_features=29, bias=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, lstm_dim=256, lstm_layers=2, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_dim, lstm_layers, dropout=dropout)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embs = self.drop(self.emb(inputs))\n",
    "        outs, (hidden, cell) = self.lstm(embs)\n",
    "        return outs, (hidden, cell)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, lstm_dim=256, lstm_layers=2, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_dim, lstm_layers, dropout=dropout)\n",
    "        self.clf = nn.Linear(lstm_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_i, state):\n",
    "        input_i = input_i.view(1, -1)          # (1, batch)\n",
    "        emb = self.drop(self.emb(input_i))     # (1, batch, emb)\n",
    "        output, state = self.lstm(emb, state)  # (1, batch, hid), ((layers, batch, hid), ...)\n",
    "        score = self.clf(output.squeeze(0)) # (batch, vocab)\n",
    "        return score, state\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        scores = []\n",
    "        _, state = self.encoder(inputs)\n",
    "        \n",
    "        for i in range(1, targets.shape[0]):\n",
    "            input_i = targets[i-1]\n",
    "            score, state = self.decoder(input_i, state)\n",
    "            scores.append(score.unsqueeze(0))\n",
    "        \n",
    "        scores = torch.cat(scores, dim=0)\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, sample, start, stop, maxlen):\n",
    "        preds = []    \n",
    "        _, state = self.encoder(sample.view(-1, 1))\n",
    "\n",
    "        token = start\n",
    "        for i in range(maxlen):\n",
    "            score, state = self.decoder(token, state)\n",
    "            token = score.argmax(dim=1)\n",
    "            if token == stop:\n",
    "                break\n",
    "            preds.append(token.item())\n",
    "        return preds\n",
    "    \n",
    "seq2seq = Seq2Seq(\n",
    "    encoder=Encoder(len(src_vocab), emb_dim=64, lstm_dim=128, lstm_layers=1), \n",
    "    decoder=Decoder(len(tgt_vocab), emb_dim=64, lstm_dim=128, lstm_layers=1)\n",
    ")\n",
    "seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq+Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqAttn(\n",
       "  (encoder): Encoder(\n",
       "    (emb): Embedding(27, 64)\n",
       "    (lstm): LSTM(64, 128, dropout=0.5)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): AttnDecoder(\n",
       "    (emb): Embedding(29, 64)\n",
       "    (lstm): LSTM(64, 128, dropout=0.5)\n",
       "    (attn): Attention(\n",
       "      (W): Linear(in_features=256, out_features=100, bias=True)\n",
       "      (v): Linear(in_features=100, out_features=1, bias=False)\n",
       "    )\n",
       "    (clf): Linear(in_features=256, out_features=29, bias=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim, attn_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, attn_dim)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, dec_hidden, enc_outs):\n",
    "        # enc_outs -> (seqlen, batch, dim)\n",
    "        # dec_hidden -> (lstm_layers, batch, hid)\n",
    "        \n",
    "        seqlen = enc_outs.size(0)\n",
    "        repeat_h = dec_hidden.repeat(seqlen, 1, 1)\n",
    "        concat_h = torch.cat((enc_outs, repeat_h), dim=2) \n",
    "        \n",
    "        scores = self.v(torch.tanh(self.W(concat_h))) # (seqlen, batch, 1)\n",
    "        probs = torch.softmax(scores, dim=0)\n",
    "        \n",
    "        weighted = enc_outs * probs # (seqlen, batch, hidden)\n",
    "        \n",
    "        context = torch.sum(weighted, dim=0, keepdim=True) # (1, batch, hidden)\n",
    "        combined = torch.cat((dec_hidden, context), dim=2)  # (1, batch, hidden*2)\n",
    "        \n",
    "        return combined\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=64, lstm_dim=256, lstm_layers=2, attn_size=100, dropout=0.5):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, lstm_dim, lstm_layers, dropout=dropout)\n",
    "        self.attn = Attention(lstm_dim * 2, attn_size)\n",
    "        self.clf = nn.Linear(lstm_dim * 2, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "    def init_state(self, batch_size, device):\n",
    "        h_0 = torch.zeros(self.lstm_layers, batch_size, self.lstm_dim).to(device)\n",
    "        c_0 = torch.zeros(self.lstm_layers, batch_size, self.lstm_dim).to(device)\n",
    "        return h_0, c_0\n",
    "    \n",
    "    def forward(self, input_i, state, enc_outs):\n",
    "        # enc_outs -> (seqlen, batch, dim)\n",
    "        \n",
    "        input_i = input_i.view(1, -1)   # (1, batch)\n",
    "        batch_size = input_i.size(1)\n",
    "        \n",
    "        emb = self.emb(input_i)         # (1, batch, emb)\n",
    "        emb = self.drop(emb)\n",
    "        \n",
    "        output, state = self.lstm(emb, state)  # (1, batch, hidden), ((layers, batch, hidden), ...)\n",
    "        combined = self.attn(output, enc_outs) # (1, batch, hidden)\n",
    "        score = self.clf(combined)          # (1, batch, vocab)\n",
    "        \n",
    "        return score, state\n",
    "    \n",
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        scores = []\n",
    "        outs, _ = self.encoder(inputs)\n",
    "        state = self.decoder.init_state(targets.shape[1], inputs.device)\n",
    "        for i in range(1, targets.shape[0]):\n",
    "            input_i = targets[i-1]\n",
    "            score, state = self.decoder(input_i, state, outs) \n",
    "            scores.append(score) # (1, batch, vocab)\n",
    "        \n",
    "        scores = torch.cat(scores, dim=0)\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, sample, start, stop, maxlen):\n",
    "        preds = []    \n",
    "        outs, _ = self.encoder(sample.view(-1, 1))\n",
    "        state = self.decoder.init_state(1, sample.device)\n",
    "        token = start\n",
    "        for i in range(maxlen):\n",
    "            score, state = self.decoder(token, state, outs)\n",
    "            token = score.argmax(dim=2)\n",
    "            if token == stop:\n",
    "                break\n",
    "            preds.append(token.item())\n",
    "        return preds\n",
    "    \n",
    "\n",
    "seq2seq_attn = Seq2SeqAttn(\n",
    "    encoder=Encoder(len(src_vocab), emb_dim=64, lstm_dim=128, lstm_layers=1, dropout=0.5), \n",
    "    decoder=AttnDecoder(len(tgt_vocab), emb_dim=64, lstm_dim=128, lstm_layers=1, attn_size=100, dropout=0.5)\n",
    ")\n",
    "seq2seq_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[19, 26, 24, 13,  8,  4, 19, 22, 25,  1],\n",
      "        [ 9,  4, 19, 13,  8, 21, 19, 12, 17,  1],\n",
      "        [16,  4, 11, 13, 16, 21, 17, 12, 17,  2],\n",
      "        [16,  1,  1,  7, 16, 21, 17, 12, 12,  2],\n",
      "        [16,  1, 21,  7, 16, 10, 19, 23, 12, 23],\n",
      "        [16, 14, 21, 24,  8, 24, 16, 23,  0, 23],\n",
      "        [16, 14, 21,  0,  8, 24, 24, 23,  0, 23],\n",
      "        [ 0, 14,  0,  0,  8, 24, 24, 24,  0, 19],\n",
      "        [ 0, 25,  0,  0,  8, 24,  0, 24,  0, 19],\n",
      "        [ 0, 23,  0,  0,  8, 24,  0,  3,  0, 19],\n",
      "        [ 0, 23,  0,  0,  8, 17,  0,  3,  0, 13],\n",
      "        [ 0,  0,  0,  0,  8, 17,  0, 22,  0, 13],\n",
      "        [ 0,  0,  0,  0, 10, 17,  0, 22,  0, 13],\n",
      "        [ 0,  0,  0,  0, 14, 17,  0, 22,  0,  6],\n",
      "        [ 0,  0,  0,  0, 14, 17,  0,  0,  0, 17],\n",
      "        [ 0,  0,  0,  0, 14,  7,  0,  0,  0,  1],\n",
      "        [ 0,  0,  0,  0,  0,  7,  0,  0,  0,  0]]) torch.Size([17, 10])\n",
      "tensor([[27, 27, 27, 27, 27, 27, 27, 27, 27, 27],\n",
      "        [ 9,  1,  1,  7,  8,  4, 16,  3, 12,  1],\n",
      "        [16,  4, 11, 13, 10,  7, 17, 12, 17,  2],\n",
      "        [19, 14, 19, 24, 14, 10, 19, 22, 25,  6],\n",
      "        [28, 23, 21, 28, 16, 17, 24, 23, 28, 13],\n",
      "        [ 0, 25, 24,  0, 28, 21, 28, 24,  0, 17],\n",
      "        [ 0, 26, 28,  0,  0, 24,  0, 28,  0, 19],\n",
      "        [ 0, 28,  0,  0,  0, 28,  0,  0,  0, 23],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0, 28]]) torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "def create_batch(x, pad_ix):\n",
    "    batch_size = len(x)\n",
    "    maxlen = max([len(xi) for xi in x])\n",
    "    batch = [xi + [pad_ix] * (maxlen - len(xi)) for xi in x]\n",
    "    batch = torch.tensor(batch)\n",
    "    return batch.transpose(0, 1).contiguous()\n",
    "\n",
    "input_batch = create_batch(train_x[:10], pad_ix=src_vocab.stoi['<pad>'])\n",
    "target_batch = create_batch(train_y[:10], pad_ix=tgt_vocab.stoi['<pad>'])\n",
    "\n",
    "print(input_batch, input_batch.shape)\n",
    "print(target_batch, target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def shuffle(x, y):\n",
    "    pack = list(zip(x, y))\n",
    "    random.shuffle(pack)\n",
    "    return zip(*pack)\n",
    "\n",
    "def train(model, inputs, targets, optimizer, criterion, config):\n",
    "    model.to(config['device'])\n",
    "    for epoch in range(1, config['epochs']+1):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        inputs, targets = shuffle(inputs, targets)\n",
    "        model.train()\n",
    "        \n",
    "        n_batches = len(inputs) // config['batch_size']\n",
    "        for batch_i in range(n_batches):\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            start_ix = config['batch_size'] * batch_i\n",
    "            end_ix = config['batch_size'] * (batch_i + 1)\n",
    "            \n",
    "            x_batch = create_batch(inputs[start_ix: end_ix], pad_ix=0).to(config['device'])\n",
    "            y_batch = create_batch(targets[start_ix: end_ix], pad_ix=0).to(config['device'])\n",
    "        \n",
    "            scores = model(x_batch, y_batch)\n",
    "            loss = criterion(scores.view(-1, scores.shape[-1]), y_batch[1:].view(-1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch} - Loss: {epoch_loss / n_batches:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.006575\n",
      "Epoch 2 - Loss: 2.489694\n",
      "Epoch 3 - Loss: 2.324506\n",
      "Epoch 4 - Loss: 2.254594\n",
      "Epoch 5 - Loss: 2.210481\n",
      "Epoch 6 - Loss: 2.166394\n",
      "Epoch 7 - Loss: 2.117718\n",
      "Epoch 8 - Loss: 2.049933\n",
      "Epoch 9 - Loss: 1.970129\n",
      "Epoch 10 - Loss: 1.874371\n",
      "Epoch 11 - Loss: 1.754994\n",
      "Epoch 12 - Loss: 1.596474\n",
      "Epoch 13 - Loss: 1.452497\n",
      "Epoch 14 - Loss: 1.322944\n",
      "Epoch 15 - Loss: 1.198761\n",
      "Epoch 16 - Loss: 1.083497\n",
      "Epoch 17 - Loss: 0.973898\n",
      "Epoch 18 - Loss: 0.869569\n",
      "Epoch 19 - Loss: 0.785726\n",
      "Epoch 20 - Loss: 0.711441\n",
      "Epoch 21 - Loss: 0.642916\n",
      "Epoch 22 - Loss: 0.584787\n",
      "Epoch 23 - Loss: 0.530618\n",
      "Epoch 24 - Loss: 0.487720\n",
      "Epoch 25 - Loss: 0.445964\n",
      "Epoch 26 - Loss: 0.410633\n",
      "Epoch 27 - Loss: 0.377951\n",
      "Epoch 28 - Loss: 0.347941\n",
      "Epoch 29 - Loss: 0.324617\n",
      "Epoch 30 - Loss: 0.301084\n",
      "Epoch 31 - Loss: 0.279771\n",
      "Epoch 32 - Loss: 0.261731\n",
      "Epoch 33 - Loss: 0.244669\n",
      "Epoch 34 - Loss: 0.225305\n",
      "Epoch 35 - Loss: 0.211502\n",
      "Epoch 36 - Loss: 0.198190\n",
      "Epoch 37 - Loss: 0.186353\n",
      "Epoch 38 - Loss: 0.172565\n",
      "Epoch 39 - Loss: 0.162860\n",
      "Epoch 40 - Loss: 0.153486\n",
      "Epoch 41 - Loss: 0.144364\n",
      "Epoch 42 - Loss: 0.134579\n",
      "Epoch 43 - Loss: 0.125286\n",
      "Epoch 44 - Loss: 0.121633\n",
      "Epoch 45 - Loss: 0.112247\n",
      "Epoch 46 - Loss: 0.106398\n",
      "Epoch 47 - Loss: 0.101564\n",
      "Epoch 48 - Loss: 0.096830\n",
      "Epoch 49 - Loss: 0.092803\n",
      "Epoch 50 - Loss: 0.087332\n",
      "Epoch 51 - Loss: 0.083948\n",
      "Epoch 52 - Loss: 0.080483\n",
      "Epoch 53 - Loss: 0.078372\n",
      "Epoch 54 - Loss: 0.072023\n",
      "Epoch 55 - Loss: 0.071205\n",
      "Epoch 56 - Loss: 0.068129\n",
      "Epoch 57 - Loss: 0.064466\n",
      "Epoch 58 - Loss: 0.059549\n",
      "Epoch 59 - Loss: 0.057331\n",
      "Epoch 60 - Loss: 0.054452\n",
      "Epoch 61 - Loss: 0.052106\n",
      "Epoch 62 - Loss: 0.050112\n",
      "Epoch 63 - Loss: 0.047754\n",
      "Epoch 64 - Loss: 0.045129\n",
      "Epoch 65 - Loss: 0.044476\n",
      "Epoch 66 - Loss: 0.042208\n",
      "Epoch 67 - Loss: 0.041133\n",
      "Epoch 68 - Loss: 0.040232\n",
      "Epoch 69 - Loss: 0.038444\n",
      "Epoch 70 - Loss: 0.037518\n",
      "Epoch 71 - Loss: 0.036978\n",
      "Epoch 72 - Loss: 0.033565\n",
      "Epoch 73 - Loss: 0.032419\n",
      "Epoch 74 - Loss: 0.033048\n",
      "Epoch 75 - Loss: 0.031003\n",
      "Epoch 76 - Loss: 0.028297\n",
      "Epoch 77 - Loss: 0.028790\n",
      "Epoch 78 - Loss: 0.027901\n",
      "Epoch 79 - Loss: 0.027461\n",
      "Epoch 80 - Loss: 0.026467\n",
      "Epoch 81 - Loss: 0.024856\n",
      "Epoch 82 - Loss: 0.025184\n",
      "Epoch 83 - Loss: 0.023872\n",
      "Epoch 84 - Loss: 0.023428\n",
      "Epoch 85 - Loss: 0.022064\n",
      "Epoch 86 - Loss: 0.021630\n",
      "Epoch 87 - Loss: 0.020359\n",
      "Epoch 88 - Loss: 0.021247\n",
      "Epoch 89 - Loss: 0.019710\n",
      "Epoch 90 - Loss: 0.020269\n",
      "Epoch 91 - Loss: 0.019684\n",
      "Epoch 92 - Loss: 0.019777\n",
      "Epoch 93 - Loss: 0.020381\n",
      "Epoch 94 - Loss: 0.018412\n",
      "Epoch 95 - Loss: 0.017573\n",
      "Epoch 96 - Loss: 0.018728\n",
      "Epoch 97 - Loss: 0.017785\n",
      "Epoch 98 - Loss: 0.017659\n",
      "Epoch 99 - Loss: 0.018119\n",
      "Epoch 100 - Loss: 0.017558\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq2seq.parameters(), lr=0.001, momentum=0.99)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean', ignore_index=tgt_vocab.stoi['<pad>'])\n",
    "      \n",
    "config = {\n",
    "    'device': torch.device('cuda:1' if torch.cuda.is_available() else 'cpu'),\n",
    "    'epochs': 100,\n",
    "    'batch_size': 50\n",
    "}\n",
    "train(seq2seq, train_x, train_y, optimizer, criterion, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'encoder': seq2seq.encoder.state_dict(),\n",
    "            'decoder': seq2seq.decoder.state_dict()}, 'seq2seq.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Prepare the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = map_many_elems(valid_inp, src_vocab.stoi)\n",
    "valid_y = map_many_elems(valid_out, tgt_vocab.stoi)\n",
    "\n",
    "valid_y = add_start_stop(tgt_vocab.stoi['<start>'], tgt_vocab.stoi['<stop>'], valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('test.txt', delimiter='\\t', header=None, usecols=[0,1])\n",
    "test_inp = df[0].tolist()\n",
    "test_out = df[1].tolist()\n",
    "\n",
    "test_x = map_many_elems(test_inp, src_vocab.stoi)\n",
    "test_y = map_many_elems(test_out, tgt_vocab.stoi)\n",
    "\n",
    "test_y = add_start_stop(tgt_vocab.stoi['<start>'], tgt_vocab.stoi['<stop>'], test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def inspect_and_eval(model, inputs, targets, src_vocab, tgt_vocab, device):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    groundtruth = []\n",
    "    \n",
    "    start = torch.tensor([tgt_vocab.stoi['<start>']]).to(device)\n",
    "    stop = torch.tensor([tgt_vocab.stoi['<stop>']]).to(device)\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        x = torch.tensor(inputs[i]).to(device)\n",
    "        preds = model.predict(x, start, stop, maxlen=len(targets[i]))\n",
    "        \n",
    "        input_str  = ''.join([src_vocab.itos[ix] for ix in inputs[i]])\n",
    "        output_str = ''.join([tgt_vocab.itos[ix] for ix in targets[i][1:-1]])\n",
    "        prediction = ''.join([tgt_vocab.itos[ix] for ix in preds])\n",
    "        \n",
    "        predictions.append(prediction)\n",
    "        groundtruth.append(output_str)\n",
    "        \n",
    "        if i < 20: \n",
    "            print(f\"{input_str} --> {output_str} --> {prediction}\")\n",
    "            \n",
    "    print(\"\\nAccuracy:\", accuracy_score(groundtruth, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sskkkwwwvkkcc --> cksvw --> cksvw\n",
      "rrqqqppyyyyzqqggg --> gpqryz --> gpqryz\n",
      "traaa --> art --> art\n",
      "yylllfflwvvw --> flvwy --> flvwy\n",
      "uurryyeeeppp --> epruy --> epruy\n",
      "apppfffnaaannnhh --> afhnp --> afhnp\n",
      "wwppllzllsjxx --> jlpswxz --> jlpswxz\n",
      "plloovaahhh --> ahlopv --> ahlopv\n",
      "oooiiuwwwjjjnn --> ijnouw --> ijnouw\n",
      "immmbgg --> bgim --> bgim\n",
      "ppmmhhhdduu --> dhmpu --> dhmpu\n",
      "iithvv --> hitv --> hitv\n",
      "aaxxwyyyxxxccce --> acewxy --> acewxy\n",
      "bbbcuuuii --> bciu --> bciu\n",
      "ddffyyygooovbbll --> bdfglovy --> bdfglovy\n",
      "ttxppiiiyyyce --> ceiptxy --> ceiptxy\n",
      "slllyy --> lsy --> lsy\n",
      "nnntttr --> nrt --> nrt\n",
      "hhbwbbnnppfffmmm --> bfhmnpw --> bfhmnpw\n",
      "sljjffk --> fjkls --> fjkls\n",
      "\n",
      "Accuracy: 0.9462\n"
     ]
    }
   ],
   "source": [
    "inspect_and_eval(seq2seq, valid_x, valid_y, src_vocab, tgt_vocab, config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwwttttjjjhhddmmmmqqffmmmdddcffu --> cdfhjmqtuw --> cdfhjmqtuw\n",
      "cccrrrvvuhwwwwwrmmm --> chmruvw --> chmruvw\n",
      "rzzzzxxxxxlyyyyykkmmmmaaaaaeeeeaahhhwwwwwiiii --> aehiklmrwxyz --> aehiklmwyz\n",
      "rrrzzpppmmmhhhh --> hmprz --> hmprz\n",
      "iiiidggggdddddmmjoonnnnnjjjy --> dgijmnoy --> dgijmnoy\n",
      "ffflllllrrrsssyybttttaa --> abflrsty --> abflrsty\n",
      "oooooooojjjjjubbbbbyyy --> bjouy --> bjouy\n",
      "cnnrrrrrlllllzqnncccrrrrrxxsssssqqqqq --> clnqrsxz --> clnqrsxz\n",
      "ppppteeeehhzzzxx --> ehptxz --> ehptxz\n",
      "fvvvvvvvggggg --> fgv --> fgv\n",
      "iiijjkkkrrrr --> ijkr --> ijkr\n",
      "uuuuujjpppppqqqqqgggggeeeeeeaasss --> aegjpqsu --> aegjpqsu\n",
      "llyyccccc --> cly --> cly\n",
      "cceewppperrrqqmmf --> cefmpqrw --> cefmpqrw\n",
      "mmiiiayyyyyzzzlllllmmm --> ailmyz --> ailmyz\n",
      "qqqfffkklllldddddggguuukkkkkcppp --> cdfgklpqu --> cdfgklpqu\n",
      "yyyyyqwwwwwazyyyyyppppz --> apqwyz --> apqwyz\n",
      "ffffbbbbfffffzzzzzrriiccccchrrrrr --> bcfhirz --> bcfhirz\n",
      "ppwwwwwwwqqqlllllmmmmcccccccccooouuu --> clmopquw --> clmopquw\n",
      "gggrrryyyyyvvvvccc --> cgrvy --> cgrvy\n",
      "\n",
      "Accuracy: 0.9006\n"
     ]
    }
   ],
   "source": [
    "inspect_and_eval(seq2seq, test_x, test_y, src_vocab, tgt_vocab, config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of the seq2seq+attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the seq2seq+attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.025384\n",
      "Epoch 2 - Loss: 2.419975\n",
      "Epoch 3 - Loss: 2.092274\n",
      "Epoch 4 - Loss: 1.798615\n",
      "Epoch 5 - Loss: 1.527555\n",
      "Epoch 6 - Loss: 1.290155\n",
      "Epoch 7 - Loss: 1.074695\n",
      "Epoch 8 - Loss: 0.882867\n",
      "Epoch 9 - Loss: 0.698329\n",
      "Epoch 10 - Loss: 0.546922\n",
      "Epoch 11 - Loss: 0.425365\n",
      "Epoch 12 - Loss: 0.330464\n",
      "Epoch 13 - Loss: 0.262080\n",
      "Epoch 14 - Loss: 0.208691\n",
      "Epoch 15 - Loss: 0.173486\n",
      "Epoch 16 - Loss: 0.151353\n",
      "Epoch 17 - Loss: 0.127625\n",
      "Epoch 18 - Loss: 0.115331\n",
      "Epoch 19 - Loss: 0.102455\n",
      "Epoch 20 - Loss: 0.093636\n",
      "Epoch 21 - Loss: 0.083018\n",
      "Epoch 22 - Loss: 0.075621\n",
      "Epoch 23 - Loss: 0.071402\n",
      "Epoch 24 - Loss: 0.068432\n",
      "Epoch 25 - Loss: 0.061416\n",
      "Epoch 26 - Loss: 0.058389\n",
      "Epoch 27 - Loss: 0.054462\n",
      "Epoch 28 - Loss: 0.053237\n",
      "Epoch 29 - Loss: 0.049783\n",
      "Epoch 30 - Loss: 0.047174\n",
      "Epoch 31 - Loss: 0.043020\n",
      "Epoch 32 - Loss: 0.043047\n",
      "Epoch 33 - Loss: 0.041037\n",
      "Epoch 34 - Loss: 0.038855\n",
      "Epoch 35 - Loss: 0.038586\n",
      "Epoch 36 - Loss: 0.036598\n",
      "Epoch 37 - Loss: 0.034978\n",
      "Epoch 38 - Loss: 0.032683\n",
      "Epoch 39 - Loss: 0.033196\n",
      "Epoch 40 - Loss: 0.031209\n",
      "Epoch 41 - Loss: 0.030770\n",
      "Epoch 42 - Loss: 0.030470\n",
      "Epoch 43 - Loss: 0.028883\n",
      "Epoch 44 - Loss: 0.028314\n",
      "Epoch 45 - Loss: 0.027497\n",
      "Epoch 46 - Loss: 0.027454\n",
      "Epoch 47 - Loss: 0.027057\n",
      "Epoch 48 - Loss: 0.026510\n",
      "Epoch 49 - Loss: 0.025960\n",
      "Epoch 50 - Loss: 0.025667\n",
      "Epoch 51 - Loss: 0.023923\n",
      "Epoch 52 - Loss: 0.024499\n",
      "Epoch 53 - Loss: 0.022592\n",
      "Epoch 54 - Loss: 0.021950\n",
      "Epoch 55 - Loss: 0.023728\n",
      "Epoch 56 - Loss: 0.021291\n",
      "Epoch 57 - Loss: 0.021014\n",
      "Epoch 58 - Loss: 0.021296\n",
      "Epoch 59 - Loss: 0.020425\n",
      "Epoch 60 - Loss: 0.020209\n",
      "Epoch 61 - Loss: 0.019680\n",
      "Epoch 62 - Loss: 0.019523\n",
      "Epoch 63 - Loss: 0.019679\n",
      "Epoch 64 - Loss: 0.018907\n",
      "Epoch 65 - Loss: 0.016756\n",
      "Epoch 66 - Loss: 0.017129\n",
      "Epoch 67 - Loss: 0.017027\n",
      "Epoch 68 - Loss: 0.017639\n",
      "Epoch 69 - Loss: 0.017225\n",
      "Epoch 70 - Loss: 0.017855\n",
      "Epoch 71 - Loss: 0.017106\n",
      "Epoch 72 - Loss: 0.016467\n",
      "Epoch 73 - Loss: 0.017056\n",
      "Epoch 74 - Loss: 0.014238\n",
      "Epoch 75 - Loss: 0.015765\n",
      "Epoch 76 - Loss: 0.015537\n",
      "Epoch 77 - Loss: 0.015487\n",
      "Epoch 78 - Loss: 0.015078\n",
      "Epoch 79 - Loss: 0.015609\n",
      "Epoch 80 - Loss: 0.014501\n",
      "Epoch 81 - Loss: 0.014784\n",
      "Epoch 82 - Loss: 0.014945\n",
      "Epoch 83 - Loss: 0.014213\n",
      "Epoch 84 - Loss: 0.013586\n",
      "Epoch 85 - Loss: 0.013850\n",
      "Epoch 86 - Loss: 0.013320\n",
      "Epoch 87 - Loss: 0.012849\n",
      "Epoch 88 - Loss: 0.013081\n",
      "Epoch 89 - Loss: 0.012226\n",
      "Epoch 90 - Loss: 0.012263\n",
      "Epoch 91 - Loss: 0.013957\n",
      "Epoch 92 - Loss: 0.013062\n",
      "Epoch 93 - Loss: 0.012841\n",
      "Epoch 94 - Loss: 0.011843\n",
      "Epoch 95 - Loss: 0.013251\n",
      "Epoch 96 - Loss: 0.012047\n",
      "Epoch 97 - Loss: 0.010822\n",
      "Epoch 98 - Loss: 0.011785\n",
      "Epoch 99 - Loss: 0.010687\n",
      "Epoch 100 - Loss: 0.010253\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(seq2seq_attn.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "train(seq2seq_attn, train_x, train_y, optimizer, criterion, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'encoder': seq2seq_attn.encoder.state_dict(),\n",
    "            'decoder': seq2seq_attn.decoder.state_dict()}, 'seq2seq_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sskkkwwwvkkcc --> cksvw --> cksvw\n",
      "rrqqqppyyyyzqqggg --> gpqryz --> gpqryz\n",
      "traaa --> art --> art\n",
      "yylllfflwvvw --> flvwy --> flvwy\n",
      "uurryyeeeppp --> epruy --> epruy\n",
      "apppfffnaaannnhh --> afhnp --> afhnp\n",
      "wwppllzllsjxx --> jlpswxz --> jlpswxz\n",
      "plloovaahhh --> ahlopv --> ahlopv\n",
      "oooiiuwwwjjjnn --> ijnouw --> ijnouw\n",
      "immmbgg --> bgim --> bgim\n",
      "ppmmhhhdduu --> dhmpu --> dhmpu\n",
      "iithvv --> hitv --> hitv\n",
      "aaxxwyyyxxxccce --> acewxy --> acewxy\n",
      "bbbcuuuii --> bciu --> bciu\n",
      "ddffyyygooovbbll --> bdfglovy --> bdfglovy\n",
      "ttxppiiiyyyce --> ceiptxy --> ceiptxy\n",
      "slllyy --> lsy --> lsy\n",
      "nnntttr --> nrt --> nrt\n",
      "hhbwbbnnppfffmmm --> bfhmnpw --> bfhmnpw\n",
      "sljjffk --> fjkls --> fjkls\n",
      "\n",
      "Accuracy: 0.9542\n"
     ]
    }
   ],
   "source": [
    "inspect_and_eval(seq2seq_attn, valid_x, valid_y, src_vocab, tgt_vocab, config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwwttttjjjhhddmmmmqqffmmmdddcffu --> cdfhjmqtuw --> cdfhjmqtuw\n",
      "cccrrrvvuhwwwwwrmmm --> chmruvw --> chmruvw\n",
      "rzzzzxxxxxlyyyyykkmmmmaaaaaeeeeaahhhwwwwwiiii --> aehiklmrwxyz --> aehiklmrwxyz\n",
      "rrrzzpppmmmhhhh --> hmprz --> hmprz\n",
      "iiiidggggdddddmmjoonnnnnjjjy --> dgijmnoy --> dgijmnoy\n",
      "ffflllllrrrsssyybttttaa --> abflrsty --> abflrsty\n",
      "oooooooojjjjjubbbbbyyy --> bjouy --> bjouy\n",
      "cnnrrrrrlllllzqnncccrrrrrxxsssssqqqqq --> clnqrsxz --> clnqrsxz\n",
      "ppppteeeehhzzzxx --> ehptxz --> ehptxz\n",
      "fvvvvvvvggggg --> fgv --> fgvv\n",
      "iiijjkkkrrrr --> ijkr --> ijkr\n",
      "uuuuujjpppppqqqqqgggggeeeeeeaasss --> aegjpqsu --> aegjpqsu\n",
      "llyyccccc --> cly --> cly\n",
      "cceewppperrrqqmmf --> cefmpqrw --> cefmpqrw\n",
      "mmiiiayyyyyzzzlllllmmm --> ailmyz --> ailmyz\n",
      "qqqfffkklllldddddggguuukkkkkcppp --> cdfgklpqu --> cdfgklpqu\n",
      "yyyyyqwwwwwazyyyyyppppz --> apqwyz --> apqwyz\n",
      "ffffbbbbfffffzzzzzrriiccccchrrrrr --> bcfhirz --> bcfhirz\n",
      "ppwwwwwwwqqqlllllmmmmcccccccccooouuu --> clmopquw --> clmopquw\n",
      "gggrrryyyyyvvvvccc --> cgrvy --> cgrvy\n",
      "\n",
      "Accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "inspect_and_eval(seq2seq_attn, test_x, test_y, src_vocab, tgt_vocab, config['device'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
